{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras import __version__\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!KERAS_BACKEND=tensorflow python3 -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_WIDTH, IM_HEIGHT = 299, 299 #fixed size for InceptionV3\n",
    "FC_SIZE = 1024\n",
    "NB_IV3_LAYERS_TO_FREEZE = 172 # unfreeze the top 2 inception blocks\n",
    "\n",
    "def setup_to_transfer_learn(model, base_model):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        print(layer.name, ': ', layer.trainable)\n",
    "    model.compile(optimizer=RMSprop(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_last_layer(base_model, nb_classes):\n",
    "    \"\"\"Add last layer to the convnet\n",
    "      Args:\n",
    "        base_model: keras model excluding top\n",
    "        nb_classes: # of classes\n",
    "      Returns:\n",
    "        new keras model with last layer\n",
    "    \"\"\"\n",
    "    \n",
    "    x = base_model.output\n",
    "\n",
    "    # GlobalAveragePooling2D converts the MxNxC tensor output into a 1xC tensor where C is the # of channels.\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(FC_SIZE, activation='relu')(x) # new FC layer, random init\n",
    "    # softmax function on the output to squeeze the values between [0,1]\n",
    "    predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_to_finetune(model):\n",
    "    \"\"\"Freeze the bottom NB_IV3_LAYERS and retrain the remaining top layers.\n",
    "  note: NB_IV3_LAYERS corresponds to the top 2 inception blocks in the inceptionv3 arch\n",
    "  Args:\n",
    "    model: keras model\n",
    "  \"\"\"\n",
    "    for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:\n",
    "        layer.trainable = True\n",
    "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Use transfer learning and fine-tuning to train a network on a new dataset\"\"\"\n",
    "    \n",
    "#     nb_train_samples = 45000\n",
    "#     nb_classes = 15\n",
    "#     nb_val_samples = 15000\n",
    "#     nb_epoch = 15\n",
    "#     batch_size = 4 #16\n",
    "#     train_dir = 'data-all/train'\n",
    "#     val_dir = 'data-all/validation'\n",
    "#     output_model_file = 'inceptionv3_clothing_expanded_classifier.h5'\n",
    "\n",
    "    nb_train_samples = 40000\n",
    "    nb_classes = 10\n",
    "    nb_val_samples = 10000\n",
    "    nb_epoch = 15\n",
    "    batch_size = 4 #16\n",
    "    train_dir = 'data/train'\n",
    "    val_dir = 'data/test'\n",
    "    output_model_file = 'inceptionv3_clothing_classifier_v1.h5'\n",
    "\n",
    "\n",
    "    # data prep\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input, # zero-centers our image data\n",
    "        rescale = 1./255,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "   \n",
    "\n",
    "    # setup model\n",
    "    # leave out the weights of the last fully connected layer\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False) #include_top=False excludes final FC layer\n",
    "    model = add_new_last_layer(base_model, nb_classes)\n",
    "    \n",
    "    \n",
    "    # transfer learning\n",
    "    setup_to_transfer_learn(model, base_model)\n",
    "\n",
    "    history_tl = model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=nb_epoch,\n",
    "        steps_per_epoch=nb_train_samples//batch_size,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_val_samples//batch_size\n",
    "    )\n",
    "    \n",
    "    print('Begin Fine Tuning')\n",
    "    # fine-tuning\n",
    "    setup_to_finetune(model)\n",
    "    \n",
    "    history_ft = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples//batch_size,\n",
    "        epochs=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_val_samples//batch_size\n",
    "    )\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    with open(\"incep_filter_clothing_classifier_v1.json\", 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    model.save(output_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43932 images belonging to 10 classes.\n",
      "Found 10022 images belonging to 10 classes.\n",
      "input_1 :  False\n",
      "conv2d_1 :  False\n",
      "batch_normalization_1 :  False\n",
      "activation_1 :  False\n",
      "conv2d_2 :  False\n",
      "batch_normalization_2 :  False\n",
      "activation_2 :  False\n",
      "conv2d_3 :  False\n",
      "batch_normalization_3 :  False\n",
      "activation_3 :  False\n",
      "max_pooling2d_1 :  False\n",
      "conv2d_4 :  False\n",
      "batch_normalization_4 :  False\n",
      "activation_4 :  False\n",
      "conv2d_5 :  False\n",
      "batch_normalization_5 :  False\n",
      "activation_5 :  False\n",
      "max_pooling2d_2 :  False\n",
      "conv2d_9 :  False\n",
      "batch_normalization_9 :  False\n",
      "activation_9 :  False\n",
      "conv2d_7 :  False\n",
      "conv2d_10 :  False\n",
      "batch_normalization_7 :  False\n",
      "batch_normalization_10 :  False\n",
      "activation_7 :  False\n",
      "activation_10 :  False\n",
      "average_pooling2d_1 :  False\n",
      "conv2d_6 :  False\n",
      "conv2d_8 :  False\n",
      "conv2d_11 :  False\n",
      "conv2d_12 :  False\n",
      "batch_normalization_6 :  False\n",
      "batch_normalization_8 :  False\n",
      "batch_normalization_11 :  False\n",
      "batch_normalization_12 :  False\n",
      "activation_6 :  False\n",
      "activation_8 :  False\n",
      "activation_11 :  False\n",
      "activation_12 :  False\n",
      "mixed0 :  False\n",
      "conv2d_16 :  False\n",
      "batch_normalization_16 :  False\n",
      "activation_16 :  False\n",
      "conv2d_14 :  False\n",
      "conv2d_17 :  False\n",
      "batch_normalization_14 :  False\n",
      "batch_normalization_17 :  False\n",
      "activation_14 :  False\n",
      "activation_17 :  False\n",
      "average_pooling2d_2 :  False\n",
      "conv2d_13 :  False\n",
      "conv2d_15 :  False\n",
      "conv2d_18 :  False\n",
      "conv2d_19 :  False\n",
      "batch_normalization_13 :  False\n",
      "batch_normalization_15 :  False\n",
      "batch_normalization_18 :  False\n",
      "batch_normalization_19 :  False\n",
      "activation_13 :  False\n",
      "activation_15 :  False\n",
      "activation_18 :  False\n",
      "activation_19 :  False\n",
      "mixed1 :  False\n",
      "conv2d_23 :  False\n",
      "batch_normalization_23 :  False\n",
      "activation_23 :  False\n",
      "conv2d_21 :  False\n",
      "conv2d_24 :  False\n",
      "batch_normalization_21 :  False\n",
      "batch_normalization_24 :  False\n",
      "activation_21 :  False\n",
      "activation_24 :  False\n",
      "average_pooling2d_3 :  False\n",
      "conv2d_20 :  False\n",
      "conv2d_22 :  False\n",
      "conv2d_25 :  False\n",
      "conv2d_26 :  False\n",
      "batch_normalization_20 :  False\n",
      "batch_normalization_22 :  False\n",
      "batch_normalization_25 :  False\n",
      "batch_normalization_26 :  False\n",
      "activation_20 :  False\n",
      "activation_22 :  False\n",
      "activation_25 :  False\n",
      "activation_26 :  False\n",
      "mixed2 :  False\n",
      "conv2d_28 :  False\n",
      "batch_normalization_28 :  False\n",
      "activation_28 :  False\n",
      "conv2d_29 :  False\n",
      "batch_normalization_29 :  False\n",
      "activation_29 :  False\n",
      "conv2d_27 :  False\n",
      "conv2d_30 :  False\n",
      "batch_normalization_27 :  False\n",
      "batch_normalization_30 :  False\n",
      "activation_27 :  False\n",
      "activation_30 :  False\n",
      "max_pooling2d_3 :  False\n",
      "mixed3 :  False\n",
      "conv2d_35 :  False\n",
      "batch_normalization_35 :  False\n",
      "activation_35 :  False\n",
      "conv2d_36 :  False\n",
      "batch_normalization_36 :  False\n",
      "activation_36 :  False\n",
      "conv2d_32 :  False\n",
      "conv2d_37 :  False\n",
      "batch_normalization_32 :  False\n",
      "batch_normalization_37 :  False\n",
      "activation_32 :  False\n",
      "activation_37 :  False\n",
      "conv2d_33 :  False\n",
      "conv2d_38 :  False\n",
      "batch_normalization_33 :  False\n",
      "batch_normalization_38 :  False\n",
      "activation_33 :  False\n",
      "activation_38 :  False\n",
      "average_pooling2d_4 :  False\n",
      "conv2d_31 :  False\n",
      "conv2d_34 :  False\n",
      "conv2d_39 :  False\n",
      "conv2d_40 :  False\n",
      "batch_normalization_31 :  False\n",
      "batch_normalization_34 :  False\n",
      "batch_normalization_39 :  False\n",
      "batch_normalization_40 :  False\n",
      "activation_31 :  False\n",
      "activation_34 :  False\n",
      "activation_39 :  False\n",
      "activation_40 :  False\n",
      "mixed4 :  False\n",
      "conv2d_45 :  False\n",
      "batch_normalization_45 :  False\n",
      "activation_45 :  False\n",
      "conv2d_46 :  False\n",
      "batch_normalization_46 :  False\n",
      "activation_46 :  False\n",
      "conv2d_42 :  False\n",
      "conv2d_47 :  False\n",
      "batch_normalization_42 :  False\n",
      "batch_normalization_47 :  False\n",
      "activation_42 :  False\n",
      "activation_47 :  False\n",
      "conv2d_43 :  False\n",
      "conv2d_48 :  False\n",
      "batch_normalization_43 :  False\n",
      "batch_normalization_48 :  False\n",
      "activation_43 :  False\n",
      "activation_48 :  False\n",
      "average_pooling2d_5 :  False\n",
      "conv2d_41 :  False\n",
      "conv2d_44 :  False\n",
      "conv2d_49 :  False\n",
      "conv2d_50 :  False\n",
      "batch_normalization_41 :  False\n",
      "batch_normalization_44 :  False\n",
      "batch_normalization_49 :  False\n",
      "batch_normalization_50 :  False\n",
      "activation_41 :  False\n",
      "activation_44 :  False\n",
      "activation_49 :  False\n",
      "activation_50 :  False\n",
      "mixed5 :  False\n",
      "conv2d_55 :  False\n",
      "batch_normalization_55 :  False\n",
      "activation_55 :  False\n",
      "conv2d_56 :  False\n",
      "batch_normalization_56 :  False\n",
      "activation_56 :  False\n",
      "conv2d_52 :  False\n",
      "conv2d_57 :  False\n",
      "batch_normalization_52 :  False\n",
      "batch_normalization_57 :  False\n",
      "activation_52 :  False\n",
      "activation_57 :  False\n",
      "conv2d_53 :  False\n",
      "conv2d_58 :  False\n",
      "batch_normalization_53 :  False\n",
      "batch_normalization_58 :  False\n",
      "activation_53 :  False\n",
      "activation_58 :  False\n",
      "average_pooling2d_6 :  False\n",
      "conv2d_51 :  False\n",
      "conv2d_54 :  False\n",
      "conv2d_59 :  False\n",
      "conv2d_60 :  False\n",
      "batch_normalization_51 :  False\n",
      "batch_normalization_54 :  False\n",
      "batch_normalization_59 :  False\n",
      "batch_normalization_60 :  False\n",
      "activation_51 :  False\n",
      "activation_54 :  False\n",
      "activation_59 :  False\n",
      "activation_60 :  False\n",
      "mixed6 :  False\n",
      "conv2d_65 :  False\n",
      "batch_normalization_65 :  False\n",
      "activation_65 :  False\n",
      "conv2d_66 :  False\n",
      "batch_normalization_66 :  False\n",
      "activation_66 :  False\n",
      "conv2d_62 :  False\n",
      "conv2d_67 :  False\n",
      "batch_normalization_62 :  False\n",
      "batch_normalization_67 :  False\n",
      "activation_62 :  False\n",
      "activation_67 :  False\n",
      "conv2d_63 :  False\n",
      "conv2d_68 :  False\n",
      "batch_normalization_63 :  False\n",
      "batch_normalization_68 :  False\n",
      "activation_63 :  False\n",
      "activation_68 :  False\n",
      "average_pooling2d_7 :  False\n",
      "conv2d_61 :  False\n",
      "conv2d_64 :  False\n",
      "conv2d_69 :  False\n",
      "conv2d_70 :  False\n",
      "batch_normalization_61 :  False\n",
      "batch_normalization_64 :  False\n",
      "batch_normalization_69 :  False\n",
      "batch_normalization_70 :  False\n",
      "activation_61 :  False\n",
      "activation_64 :  False\n",
      "activation_69 :  False\n",
      "activation_70 :  False\n",
      "mixed7 :  False\n",
      "conv2d_73 :  False\n",
      "batch_normalization_73 :  False\n",
      "activation_73 :  False\n",
      "conv2d_74 :  False\n",
      "batch_normalization_74 :  False\n",
      "activation_74 :  False\n",
      "conv2d_71 :  False\n",
      "conv2d_75 :  False\n",
      "batch_normalization_71 :  False\n",
      "batch_normalization_75 :  False\n",
      "activation_71 :  False\n",
      "activation_75 :  False\n",
      "conv2d_72 :  False\n",
      "conv2d_76 :  False\n",
      "batch_normalization_72 :  False\n",
      "batch_normalization_76 :  False\n",
      "activation_72 :  False\n",
      "activation_76 :  False\n",
      "max_pooling2d_4 :  False\n",
      "mixed8 :  False\n",
      "conv2d_81 :  False\n",
      "batch_normalization_81 :  False\n",
      "activation_81 :  False\n",
      "conv2d_78 :  False\n",
      "conv2d_82 :  False\n",
      "batch_normalization_78 :  False\n",
      "batch_normalization_82 :  False\n",
      "activation_78 :  False\n",
      "activation_82 :  False\n",
      "conv2d_79 :  False\n",
      "conv2d_80 :  False\n",
      "conv2d_83 :  False\n",
      "conv2d_84 :  False\n",
      "average_pooling2d_8 :  False\n",
      "conv2d_77 :  False\n",
      "batch_normalization_79 :  False\n",
      "batch_normalization_80 :  False\n",
      "batch_normalization_83 :  False\n",
      "batch_normalization_84 :  False\n",
      "conv2d_85 :  False\n",
      "batch_normalization_77 :  False\n",
      "activation_79 :  False\n",
      "activation_80 :  False\n",
      "activation_83 :  False\n",
      "activation_84 :  False\n",
      "batch_normalization_85 :  False\n",
      "activation_77 :  False\n",
      "mixed9_0 :  False\n",
      "concatenate_1 :  False\n",
      "activation_85 :  False\n",
      "mixed9 :  False\n",
      "conv2d_90 :  False\n",
      "batch_normalization_90 :  False\n",
      "activation_90 :  False\n",
      "conv2d_87 :  False\n",
      "conv2d_91 :  False\n",
      "batch_normalization_87 :  False\n",
      "batch_normalization_91 :  False\n",
      "activation_87 :  False\n",
      "activation_91 :  False\n",
      "conv2d_88 :  False\n",
      "conv2d_89 :  False\n",
      "conv2d_92 :  False\n",
      "conv2d_93 :  False\n",
      "average_pooling2d_9 :  False\n",
      "conv2d_86 :  False\n",
      "batch_normalization_88 :  False\n",
      "batch_normalization_89 :  False\n",
      "batch_normalization_92 :  False\n",
      "batch_normalization_93 :  False\n",
      "conv2d_94 :  False\n",
      "batch_normalization_86 :  False\n",
      "activation_88 :  False\n",
      "activation_89 :  False\n",
      "activation_92 :  False\n",
      "activation_93 :  False\n",
      "batch_normalization_94 :  False\n",
      "activation_86 :  False\n",
      "mixed9_1 :  False\n",
      "concatenate_2 :  False\n",
      "activation_94 :  False\n",
      "mixed10 :  False\n",
      "global_average_pooling2d_1 :  True\n",
      "dense_1 :  True\n",
      "dense_2 :  True\n",
      "Epoch 1/15\n",
      "10000/10000 [==============================] - 1017s 102ms/step - loss: 1.6287 - acc: 0.4546 - val_loss: 1.1399 - val_acc: 0.5923\n",
      "Epoch 2/15\n",
      "10000/10000 [==============================] - 1005s 101ms/step - loss: 1.3337 - acc: 0.5358 - val_loss: 1.0003 - val_acc: 0.6405\n",
      "Epoch 3/15\n",
      "10000/10000 [==============================] - 1010s 101ms/step - loss: 1.2681 - acc: 0.5582 - val_loss: 0.9782 - val_acc: 0.6506\n",
      "Epoch 4/15\n",
      "10000/10000 [==============================] - 1009s 101ms/step - loss: 1.2359 - acc: 0.5706 - val_loss: 0.9691 - val_acc: 0.6645\n",
      "Epoch 5/15\n",
      "10000/10000 [==============================] - 1009s 101ms/step - loss: 1.2189 - acc: 0.5791 - val_loss: 0.9773 - val_acc: 0.6586\n",
      "Epoch 6/15\n",
      "10000/10000 [==============================] - 1006s 101ms/step - loss: 1.1900 - acc: 0.5889 - val_loss: 0.9745 - val_acc: 0.6673\n",
      "Epoch 7/15\n",
      "10000/10000 [==============================] - 1001s 100ms/step - loss: 1.1903 - acc: 0.5837 - val_loss: 0.9369 - val_acc: 0.6786\n",
      "Epoch 8/15\n",
      "10000/10000 [==============================] - 994s 99ms/step - loss: 1.1700 - acc: 0.5933 - val_loss: 0.9568 - val_acc: 0.6747\n",
      "Epoch 9/15\n",
      "10000/10000 [==============================] - 1006s 101ms/step - loss: 1.1612 - acc: 0.5947 - val_loss: 0.9835 - val_acc: 0.6700\n",
      "Epoch 10/15\n",
      "10000/10000 [==============================] - 1008s 101ms/step - loss: 1.1458 - acc: 0.6019 - val_loss: 0.9320 - val_acc: 0.6894\n",
      "Epoch 11/15\n",
      "10000/10000 [==============================] - 987s 99ms/step - loss: 1.1450 - acc: 0.6003 - val_loss: 0.9332 - val_acc: 0.6900\n",
      "Epoch 12/15\n",
      "10000/10000 [==============================] - 990s 99ms/step - loss: 1.1307 - acc: 0.6060 - val_loss: 0.9385 - val_acc: 0.6877\n",
      "Epoch 13/15\n",
      "10000/10000 [==============================] - 989s 99ms/step - loss: 1.1313 - acc: 0.6039 - val_loss: 0.9705 - val_acc: 0.6794\n",
      "Epoch 14/15\n",
      "10000/10000 [==============================] - 975s 97ms/step - loss: 1.1264 - acc: 0.6064 - val_loss: 0.9858 - val_acc: 0.6711\n",
      "Epoch 15/15\n",
      "10000/10000 [==============================] - 986s 99ms/step - loss: 1.1095 - acc: 0.6143 - val_loss: 0.9533 - val_acc: 0.6892\n",
      "Begin Fine Tuning\n",
      "Epoch 1/15\n",
      "10000/10000 [==============================] - 1016s 102ms/step - loss: 1.0520 - acc: 0.6503 - val_loss: 0.6979 - val_acc: 0.7598\n",
      "Epoch 2/15\n",
      "10000/10000 [==============================] - 1000s 100ms/step - loss: 0.6964 - acc: 0.7580 - val_loss: 0.6570 - val_acc: 0.7739\n",
      "Epoch 3/15\n",
      "10000/10000 [==============================] - 1007s 101ms/step - loss: 0.6059 - acc: 0.7891 - val_loss: 0.6082 - val_acc: 0.7935\n",
      "Epoch 4/15\n",
      "10000/10000 [==============================] - 992s 99ms/step - loss: 0.5471 - acc: 0.8076 - val_loss: 0.6178 - val_acc: 0.8000\n",
      "Epoch 5/15\n",
      "10000/10000 [==============================] - 997s 100ms/step - loss: 0.5084 - acc: 0.8243 - val_loss: 0.5960 - val_acc: 0.8021\n",
      "Epoch 6/15\n",
      "10000/10000 [==============================] - 1008s 101ms/step - loss: 0.4784 - acc: 0.8338 - val_loss: 0.5573 - val_acc: 0.8134\n",
      "Epoch 7/15\n",
      "10000/10000 [==============================] - 1009s 101ms/step - loss: 0.4623 - acc: 0.8405 - val_loss: 0.5606 - val_acc: 0.8123\n",
      "Epoch 8/15\n",
      "10000/10000 [==============================] - 1008s 101ms/step - loss: 0.4401 - acc: 0.8471 - val_loss: 0.5735 - val_acc: 0.8129\n",
      "Epoch 9/15\n",
      "10000/10000 [==============================] - 1002s 100ms/step - loss: 0.4203 - acc: 0.8543 - val_loss: 0.5840 - val_acc: 0.8155\n",
      "Epoch 10/15\n",
      "10000/10000 [==============================] - 987s 99ms/step - loss: 0.4051 - acc: 0.8587 - val_loss: 0.6177 - val_acc: 0.8044\n",
      "Epoch 11/15\n",
      "10000/10000 [==============================] - 1009s 101ms/step - loss: 0.3943 - acc: 0.8636 - val_loss: 0.5566 - val_acc: 0.8271\n",
      "Epoch 12/15\n",
      "10000/10000 [==============================] - 985s 98ms/step - loss: 0.3781 - acc: 0.8672 - val_loss: 0.5271 - val_acc: 0.8373\n",
      "Epoch 13/15\n",
      "10000/10000 [==============================] - 1006s 101ms/step - loss: 0.3701 - acc: 0.8698 - val_loss: 0.5635 - val_acc: 0.8296\n",
      "Epoch 14/15\n",
      "10000/10000 [==============================] - 1010s 101ms/step - loss: 0.3580 - acc: 0.8757 - val_loss: 0.5549 - val_acc: 0.8267\n",
      "Epoch 15/15\n",
      "10000/10000 [==============================] - 1007s 101ms/step - loss: 0.3445 - acc: 0.8791 - val_loss: 0.5609 - val_acc: 0.8294\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
